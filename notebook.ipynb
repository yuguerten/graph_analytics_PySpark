{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763d2ca1",
   "metadata": {},
   "source": [
    "Graph Analytics with Apache Spark and GraphFrames\n",
    "Interactive Practical Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e82e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df46015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please make sure GraphFrames is installed in your Spark environment.\n",
    "# Install GraphFrames if necessary:\n",
    "! pip install graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a838fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "from graphframes import GraphFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83690ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/25 20:29:16 WARN Utils: Your hostname, mouhcine resolves to a loopback address: 127.0.1.1; using 192.168.11.103 instead (on interface wlo1)\n",
      "24/10/25 20:29:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/yuguerten/.ivy2/cache\n",
      "The jars for the packages stored in: /home/yuguerten/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-42f910af-ae15-4a01-af4c-41d27e56fa76;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/yuguerten/studies/Semestre_3/Big_data_2/Tps/tp4/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 104ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-42f910af-ae15-4a01-af4c-41d27e56fa76\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "24/10/25 20:29:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/25 20:29:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GraphFrames Example\") \\\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1134df2a",
   "metadata": {},
   "source": [
    "## Task 1: Load the Data\n",
    "Load the station and trip data as DataFrames\n",
    "Question: Inspect the loaded data and understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c5ec11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load station data\n",
    "station_data_path = \"data/201508_station_data.csv\"\n",
    "bikeStations = spark.read.option(\"header\", \"true\").csv(station_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4340dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trip data\n",
    "trip_data_path = \"data/201508_trip_data.csv\"\n",
    "tripData = spark.read.option(\"header\", \"true\").csv(trip_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da5abff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station Data:\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "|         2|San Jose Diridon ...|37.329732|-121.901782|       27|San Jose|    8/6/2013|\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|San Jose|    8/5/2013|\n",
      "|         4|Santa Clara at Al...|37.333988|-121.894902|       11|San Jose|    8/6/2013|\n",
      "|         5|    Adobe on Almaden|37.331415|  -121.8932|       19|San Jose|    8/5/2013|\n",
      "|         6|    San Pedro Square|37.336721|-121.894074|       15|San Jose|    8/7/2013|\n",
      "+----------+--------------------+---------+-----------+---------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Trip Data:\n",
      "+-------+--------+---------------+--------------------+--------------+---------------+--------------------+------------+------+---------------+--------+\n",
      "|Trip ID|Duration|     Start Date|       Start Station|Start Terminal|       End Date|         End Station|End Terminal|Bike #|Subscriber Type|Zip Code|\n",
      "+-------+--------+---------------+--------------------+--------------+---------------+--------------------+------------+------+---------------+--------+\n",
      "| 913460|     765|8/31/2015 23:26|Harry Bridges Pla...|            50|8/31/2015 23:39|San Francisco Cal...|          70|   288|     Subscriber|    2139|\n",
      "| 913459|    1036|8/31/2015 23:11|San Antonio Shopp...|            31|8/31/2015 23:28|Mountain View Cit...|          27|    35|     Subscriber|   95032|\n",
      "| 913455|     307|8/31/2015 23:13|      Post at Kearny|            47|8/31/2015 23:18|   2nd at South Park|          64|   468|     Subscriber|   94107|\n",
      "| 913454|     409|8/31/2015 23:10|  San Jose City Hall|            10|8/31/2015 23:17| San Salvador at 1st|           8|    68|     Subscriber|   95113|\n",
      "| 913453|     789|8/31/2015 23:09|Embarcadero at Fo...|            51|8/31/2015 23:22|Embarcadero at Sa...|          60|   487|       Customer|    9069|\n",
      "+-------+--------+---------------+--------------------+--------------+---------------+--------------------+------------+------+---------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of each DataFrame\n",
    "print(\"Station Data:\")\n",
    "bikeStations.show(5)\n",
    "print(\"Trip Data:\")\n",
    "tripData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d32aab",
   "metadata": {},
   "source": [
    "## Task 2: Define Vertices and Edges\n",
    "Prepare vertices and edges for the graph using the GraphFrames conventions.\n",
    "Rename columns to match GraphFrames requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7482e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "stationVertices = bikeStations.withColumnRenamed(\"name\", \"id\").distinct()\n",
    "tripEdges = tripData \\\n",
    "    .withColumnRenamed(\"Start Station\", \"src\") \\\n",
    "    .withColumnRenamed(\"End Station\", \"dst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2733102e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertices (Stations):\n",
      "+----------+--------------------+---------+-----------+---------+-------------+------------+\n",
      "|station_id|                  id|      lat|       long|dockcount|     landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+-------------+------------+\n",
      "|        51|Embarcadero at Fo...|37.791464|-122.391034|       19|San Francisco|   8/20/2013|\n",
      "|        58|San Francisco Cit...| 37.77865|-122.418235|       19|San Francisco|   8/21/2013|\n",
      "|        60|Embarcadero at Sa...| 37.80477|-122.403234|       15|San Francisco|   8/21/2013|\n",
      "|        65|     Townsend at 7th|37.771058|-122.402717|       15|San Francisco|   8/22/2013|\n",
      "|        63|       Howard at 2nd|37.786978|-122.398108|       19|San Francisco|   8/22/2013|\n",
      "+----------+--------------------+---------+-----------+---------+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Edges (Trips):\n",
      "+-------+--------+---------------+--------------------+--------------+---------------+--------------------+------------+------+---------------+--------+\n",
      "|Trip ID|Duration|     Start Date|                 src|Start Terminal|       End Date|                 dst|End Terminal|Bike #|Subscriber Type|Zip Code|\n",
      "+-------+--------+---------------+--------------------+--------------+---------------+--------------------+------------+------+---------------+--------+\n",
      "| 913460|     765|8/31/2015 23:26|Harry Bridges Pla...|            50|8/31/2015 23:39|San Francisco Cal...|          70|   288|     Subscriber|    2139|\n",
      "| 913459|    1036|8/31/2015 23:11|San Antonio Shopp...|            31|8/31/2015 23:28|Mountain View Cit...|          27|    35|     Subscriber|   95032|\n",
      "| 913455|     307|8/31/2015 23:13|      Post at Kearny|            47|8/31/2015 23:18|   2nd at South Park|          64|   468|     Subscriber|   94107|\n",
      "| 913454|     409|8/31/2015 23:10|  San Jose City Hall|            10|8/31/2015 23:17| San Salvador at 1st|           8|    68|     Subscriber|   95113|\n",
      "| 913453|     789|8/31/2015 23:09|Embarcadero at Fo...|            51|8/31/2015 23:22|Embarcadero at Sa...|          60|   487|       Customer|    9069|\n",
      "+-------+--------+---------------+--------------------+--------------+---------------+--------------------+------------+------+---------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the prepared vertices and edges\n",
    "print(\"Vertices (Stations):\")\n",
    "stationVertices.show(5)\n",
    "print(\"Edges (Trips):\")\n",
    "tripEdges.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd3ceb9",
   "metadata": {},
   "source": [
    "## Task 3: Construct the Graph\n",
    "Use the GraphFrames library to create a graph using the vertices and edges dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a3dbab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphFrame(v:[id: string, station_id: string ... 5 more fields], e:[src: string, dst: string ... 9 more fields])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stationGraph = GraphFrame(stationVertices, tripEdges)\n",
    "stationGraph.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f9cb8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Stations: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Trips in Graph: 354152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Confirm graph statistics\n",
    "print(\"Total Number of Stations:\", stationGraph.vertices.count())\n",
    "print(\"Total Number of Trips in Graph:\", stationGraph.edges.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9329b7a9",
   "metadata": {},
   "source": [
    "## Task 4: Basic Graph Queries\n",
    "Count the number of trips between popular stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3f26828",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_trips = stationGraph.edges \\\n",
    "    .groupBy(\"src\", \"dst\") \\\n",
    "    .count() \\\n",
    "    .orderBy(desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c6e5d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Popular Trips:\n",
      "+--------------------+--------------------+-----+\n",
      "|                 src|                 dst|count|\n",
      "+--------------------+--------------------+-----+\n",
      "|San Francisco Cal...|     Townsend at 7th| 3748|\n",
      "|Harry Bridges Pla...|Embarcadero at Sa...| 3145|\n",
      "|     2nd at Townsend|Harry Bridges Pla...| 2973|\n",
      "|     Townsend at 7th|San Francisco Cal...| 2734|\n",
      "|Harry Bridges Pla...|     2nd at Townsend| 2640|\n",
      "|Embarcadero at Fo...|San Francisco Cal...| 2439|\n",
      "|   Steuart at Market|     2nd at Townsend| 2356|\n",
      "|Embarcadero at Sa...|   Steuart at Market| 2330|\n",
      "|     Townsend at 7th|San Francisco Cal...| 2192|\n",
      "|Temporary Transba...|San Francisco Cal...| 2184|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the top 10 popular trips\n",
    "print(\"Top 10 Popular Trips:\")\n",
    "popular_trips.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a9e6ae",
   "metadata": {},
   "source": [
    "QUESTION: Modify the query to filter trips from or to a specific station."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5230280b",
   "metadata": {},
   "source": [
    "## Task 5: PageRank Calculation\n",
    "Use PageRank to identify influential stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84852d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/25 20:40:55 WARN MemoryStore: Not enough space to cache rdd_148_2 in memory! (computed 30.7 MiB so far)\n",
      "24/10/25 20:40:55 WARN BlockManager: Block rdd_148_2 could not be removed as it was not found on disk or in memory\n",
      "24/10/25 20:40:55 WARN BlockManager: Putting block rdd_148_2 failed\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Stations by PageRank:\n",
      "+--------------------+------------------+\n",
      "|                  id|          pagerank|\n",
      "+--------------------+------------------+\n",
      "|San Jose Diridon ...| 4.051504835989957|\n",
      "|San Francisco Cal...| 3.351183296428704|\n",
      "|Mountain View Cal...| 2.514390771015558|\n",
      "|Redwood City Calt...|2.3263087713711688|\n",
      "|San Francisco Cal...|2.2311442913698567|\n",
      "|Harry Bridges Pla...|1.8251120118882902|\n",
      "|     2nd at Townsend|1.5821217785039197|\n",
      "|Santa Clara at Al...| 1.573007408490752|\n",
      "|     Townsend at 7th| 1.568456580534067|\n",
      "|Embarcadero at Sa...|1.5414242087748948|\n",
      "+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ranks = stationGraph.pageRank(resetProbability=0.15, maxIter=10)\n",
    "# Display stations with highest PageRank values\n",
    "print(\"Top 10 Stations by PageRank:\")\n",
    "ranks.vertices.orderBy(desc(\"pagerank\")).select(\"id\", \"pagerank\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174dae40",
   "metadata": {},
   "source": [
    "QUESTION: Try adjusting the `resetProbability` and `maxIter` parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9abf0d",
   "metadata": {},
   "source": [
    "## Task 6: In-Degree and Out-Degree Analysis\n",
    "Calculate the in-degree and out-degree for each station.\n",
    "In-degree represents incoming trips, out-degree represents outgoing trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2d7242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inDeg = stationGraph.inDegrees\n",
    "outDeg = stationGraph.outDegrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a343c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Stations by In-Degree:\n",
      "+--------------------+--------+\n",
      "|                  id|inDegree|\n",
      "+--------------------+--------+\n",
      "|San Francisco Cal...|   34810|\n",
      "|San Francisco Cal...|   22523|\n",
      "|Harry Bridges Pla...|   17810|\n",
      "|     2nd at Townsend|   15463|\n",
      "|     Townsend at 7th|   15422|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display stations with the highest in-degree and out-degree\n",
    "print(\"Top 5 Stations by In-Degree:\")\n",
    "inDeg.orderBy(desc(\"inDegree\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66e93276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Stations by Out-Degree:\n",
      "+--------------------+---------+\n",
      "|                  id|outDegree|\n",
      "+--------------------+---------+\n",
      "|San Francisco Cal...|    26304|\n",
      "|San Francisco Cal...|    21758|\n",
      "|Harry Bridges Pla...|    17255|\n",
      "|Temporary Transba...|    14436|\n",
      "|Embarcadero at Sa...|    14158|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 5 Stations by Out-Degree:\")\n",
    "outDeg.orderBy(desc(\"outDegree\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6795b052",
   "metadata": {},
   "source": [
    "QUESTION: What does the degree ratio (inDegree/outDegree) tell us about the stationâ€™s activity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffdeafa",
   "metadata": {},
   "source": [
    "## Task 7: Breadth-First Search (BFS)\n",
    "Use BFS to find shortest paths between two stations.\n",
    "Set `fromExpr` and `toExpr` to identify specific stations for the BFS traversal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64373f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bfs_result = stationGraph.bfs(\n",
    "    fromExpr=\"id = 'Townsend at 7th'\",\n",
    "    toExpr=\"id = 'Spear at Folsom'\",\n",
    "    maxPathLength=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67001c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BFS Path from 'Townsend at 7th' to 'Spear at Folsom':\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                from|                  e0|                  to|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|{65, Townsend at ...|{913371, 663, 8/3...|{49, Spear at Fol...|\n",
      "|{65, Townsend at ...|{913265, 658, 8/3...|{49, Spear at Fol...|\n",
      "|{65, Townsend at ...|{911919, 722, 8/3...|{49, Spear at Fol...|\n",
      "|{65, Townsend at ...|{910777, 704, 8/2...|{49, Spear at Fol...|\n",
      "|{65, Townsend at ...|{908994, 1115, 8/...|{49, Spear at Fol...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the BFS result\n",
    "print(\"BFS Path from 'Townsend at 7th' to 'Spear at Folsom':\")\n",
    "bfs_result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ab272",
   "metadata": {},
   "source": [
    "QUESTION: Experiment with different stations to find paths between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695189a0",
   "metadata": {},
   "source": [
    "## Task 8: Connected Components\n",
    "Use connected components to find subgraphs with connected stations.\n",
    "This method assumes an undirected graph. Checkpointing is necessary for long processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e46d218d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/25 20:43:40 WARN CacheManager: Asked to cache already cached data.\n",
      "[Stage 789:==========================================>          (160 + 9) / 200]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o158.run.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:101)\n\tat java.base/java.lang.StringBuilder.<init>(StringBuilder.java:119)\n\tat org.apache.spark.sql.catalyst.util.StringConcat.toString(StringUtils.scala:62)\n\tat org.apache.spark.sql.catalyst.util.StringUtils$PlanStringConcat.toString(StringUtils.scala:152)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:254)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.onUpdatePlan(AdaptiveSparkPlanExec.scala:777)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$9(AdaptiveSparkPlanExec.scala:373)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda/0x0000702c40fdfa30.apply$mcVJ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:373)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda/0x0000702c40f717f8.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:417)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)\n\tat org.apache.spark.sql.Dataset$$Lambda/0x0000702c4113d4b8.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.Dataset$$Lambda/0x0000702c40d2fd30.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset$$Lambda/0x0000702c40c4e3a0.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda/0x0000702c40c51c08.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda/0x0000702c40c4e668.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetCheckpointDir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmp/checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m sampledGraph \u001b[38;5;241m=\u001b[39m GraphFrame(stationVertices, tripEdges\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m0.1\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m cc \u001b[38;5;241m=\u001b[39m \u001b[43msampledGraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnectedComponents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/studies/Semestre_3/Big_data_2/Tps/tp4/.venv/lib/python3.12/site-packages/graphframes/graphframe.py:319\u001b[0m, in \u001b[0;36mGraphFrame.connectedComponents\u001b[0;34m(self, algorithm, checkpointInterval, broadcastThreshold)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnectedComponents\u001b[39m(\u001b[38;5;28mself\u001b[39m, algorithm \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraphframes\u001b[39m\u001b[38;5;124m\"\u001b[39m, checkpointInterval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    301\u001b[0m                         broadcastThreshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000000\u001b[39m):\n\u001b[1;32m    302\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m    Computes the connected components of the graph.\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    :return: DataFrame with new vertices column \"component\"\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnectedComponents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetAlgorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetCheckpointInterval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpointInterval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetBroadcastThreshold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbroadcastThreshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m--> 319\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sqlContext)\n",
      "File \u001b[0;32m~/studies/Semestre_3/Big_data_2/Tps/tp4/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/studies/Semestre_3/Big_data_2/Tps/tp4/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/studies/Semestre_3/Big_data_2/Tps/tp4/.venv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o158.run.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:101)\n\tat java.base/java.lang.StringBuilder.<init>(StringBuilder.java:119)\n\tat org.apache.spark.sql.catalyst.util.StringConcat.toString(StringUtils.scala:62)\n\tat org.apache.spark.sql.catalyst.util.StringUtils$PlanStringConcat.toString(StringUtils.scala:152)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:254)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.onUpdatePlan(AdaptiveSparkPlanExec.scala:777)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$9(AdaptiveSparkPlanExec.scala:373)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda/0x0000702c40fdfa30.apply$mcVJ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:373)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda/0x0000702c40f717f8.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:417)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)\n\tat org.apache.spark.sql.Dataset$$Lambda/0x0000702c4113d4b8.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.Dataset$$Lambda/0x0000702c40d2fd30.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset$$Lambda/0x0000702c40c4e3a0.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda/0x0000702c40c51c08.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda/0x0000702c40c4e668.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.setCheckpointDir(\"tmp/checkpoints\")\n",
    "sampledGraph = GraphFrame(stationVertices, tripEdges.sample(False, 0.1))\n",
    "cc = sampledGraph.connectedComponents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1040417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display connected components\n",
    "print(\"Connected Components:\")\n",
    "cc.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1860e4c1",
   "metadata": {},
   "source": [
    "QUESTION: How many unique connected components are found? Does the result align with your expectation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7149a1",
   "metadata": {},
   "source": [
    "End of Practical Notebook Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d643b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session after the practical session is complete.\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d4cbfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b21fabe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3971f147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21569649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
